{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ab031231fea192",
   "metadata": {},
   "source": [
    "# A Simple Experiment \n",
    "in which we try to predict the difference in molecular weight from a set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5f9de3a5bc20e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import rdkit.Chem\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn import svm, metrics, clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec3f767a86184a",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af213db402ea9c6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mol_path = Path(\"/data/chodera/asap-datasets/mpro_fragalysis-04-01-24_curated_cache/combined_2d.sdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb83254e6710213",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mols = [mol for mol in Chem.SDMolSupplier(str(mol_path))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9500d0db0f8464",
   "metadata": {},
   "source": [
    "# Calculate molecular weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef5133c37ba6e420",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mol_weight = [Descriptors.MolWt(mol) for mol in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1e2c2dd177e2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d68bc0deeb655e4",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2dede0c1d2014b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mol1 = [mol for mol in mols for _ in mols]\n",
    "weight1 = [weight for weight in mol_weight for _ in mol_weight]\n",
    "mol2 = [mol for _ in mols for mol in mols]\n",
    "weight2 = [weight for _ in mol_weight for weight in mol_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843409588ede0e47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"mol1\": mol1, \"mol2\": mol2, \"weight1\": weight1, \"weight2\": weight2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e8af55db336a60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"weight_diff\"] = np.abs(df[\"weight1\"] - df[\"weight2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9de306fd8a6b1",
   "metadata": {},
   "source": [
    "# methods for extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c5088d6b2f4f5a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def mol_to_fp(mol: rdkit.Chem.Mol, method=\"maccs\", n_bits=2048):\n",
    "    from rdkit.Chem import MACCSkeys, rdFingerprintGenerator\n",
    "    \"\"\"\n",
    "    Encode a molecule from a SMILES string into a fingerprint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        The SMILES string defining the molecule.\n",
    "\n",
    "    method : str\n",
    "        The type of fingerprint to use. Default is MACCS keys.\n",
    "\n",
    "    n_bits : int\n",
    "        The length of the fingerprint.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        The fingerprint array.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if method == \"maccs\":\n",
    "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
    "    if method == \"morgan2\":\n",
    "        fpg = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=n_bits)\n",
    "        return np.array(fpg.GetFingerprint(mol))\n",
    "    if method == \"morgan3\":\n",
    "        fpg = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=n_bits)\n",
    "        return np.array(fpg.GetFingerprint(mol))\n",
    "    else:\n",
    "        # NBVAL_CHECK_OUTPUT\n",
    "        print(f\"Warning: Wrong method specified: {method}. Default will be used instead.\")\n",
    "        return np.array(MACCSkeys.GenMACCSKeys(mol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d90c7f7c23f0a4ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fps = [mol_to_fp(mol) for mol in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4161ca99e586973",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"maccs1\"] = [fp for fp in fps for _ in fps]\n",
    "df[\"maccs2\"] = [fp for _ in fps for fp in fps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825e3e38e9432c18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ecfp4 = [mol_to_fp(mol, method=\"morgan2\") for mol in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bdfc87be27ee848",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"morgan2_1\"] = [fp for fp in ecfp4 for _ in ecfp4]\n",
    "df[\"morgan2_2\"] = [fp for _ in ecfp4 for fp in ecfp4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1700d4f8563817ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"combined_maccs\"] = df.apply(lambda x: np.concatenate([x[\"maccs1\"], x[\"maccs2\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770ff2986a50a3d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"combined_morgan2\"] = df.apply(lambda x: np.concatenate([x[\"morgan2_1\"], x[\"morgan2_2\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde5ad6ad5ea766",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd6f293e49265d9a",
   "metadata": {},
   "source": [
    "# Define ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9b5eedb5a2807aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# actually we can't use a RF classifier because the thing we're trying to predict is continuous\n",
    "# # Set model parameter for random forest\n",
    "# param = {\n",
    "#     \"n_estimators\": 100,  # number of trees to grows\n",
    "#     \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
    "# }\n",
    "# model_RF = RandomForestClassifier(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae5f4540699a125",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_SVR = svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb40ffee4873b01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95f93efee402d09b",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41a692378f4e8fad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate the performance of a machine learning model on a test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ml_model: sklearn model object\n",
    "        The machine learning model to evaluate.\n",
    "    test_x: array\n",
    "        The test set descriptors.\n",
    "    test_y: array\n",
    "        The test set labels.\n",
    "    verbose: bool\n",
    "        Print performance info (default = True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple:\n",
    "        Accuracy, sensitivity, specificity, auc on test set.\n",
    "\n",
    "    \"\"\"\n",
    "    # Predict the test set\n",
    "    pred_y = ml_model.predict(test_x)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_y, pred_y)\n",
    "\n",
    "    # Calculate the sensitivity\n",
    "    sens = sklearn.metrics.recall_score(test_y, pred_y)\n",
    "\n",
    "    # Calculate the specificity\n",
    "    spec = sklearn.metrics.recall_score(test_y, pred_y, pos_label=0)\n",
    "\n",
    "    # Calculate the AUC\n",
    "    auc = sklearn.metrics.roc_auc_score(test_y, pred_y)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Sensitivity: {sens:.2f}\")\n",
    "        print(f\"Specificity: {spec:.2f}\")\n",
    "        print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "    return accuracy, sens, spec, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bc4ff9772cdb3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edddbc67186d5d37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def model_training_and_validation(ml_model, name, splits, verbose=True):\n",
    "    \"\"\"\n",
    "    Fit a machine learning model on a random train-test split of the data\n",
    "    and return the performance measures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ml_model: sklearn model object\n",
    "        The machine learning model to train.\n",
    "    name: str\n",
    "        Name of machine learning algorithm: RF, SVM, ANN\n",
    "    splits: list\n",
    "        List of desciptor and label data: train_x, test_x, train_y, test_y.\n",
    "    verbose: bool\n",
    "        Print performance info (default = True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple:\n",
    "        Accuracy, sensitivity, specificity, auc on test set.\n",
    "\n",
    "    \"\"\"\n",
    "    train_x, test_x, train_y, test_y = splits\n",
    "\n",
    "    # Fit the model\n",
    "    ml_model.fit(train_x, train_y)\n",
    "\n",
    "    # Calculate model performance results\n",
    "    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)\n",
    "\n",
    "    return accuracy, sens, spec, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c572acb6537fdc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8732e1085067aaa9",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d27b97f5c60c22ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 8405\n",
      "Test data size: 33620\n"
     ]
    }
   ],
   "source": [
    "fingerprint_to_model = df.combined_morgan2.tolist()\n",
    "label_to_model = df.weight_diff.tolist()\n",
    "\n",
    "# Split data randomly in train and test set\n",
    "# note that we use test/train_x for the respective fingerprint splits\n",
    "# and test/train_y for the respective label splits\n",
    "(\n",
    "    static_train_x,\n",
    "    static_test_x,\n",
    "    static_train_y,\n",
    "    static_test_y,\n",
    ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.8)\n",
    "splits = [static_train_x, static_test_x, static_train_y, static_test_y]\n",
    "# NBVAL_CHECK_OUTPUT\n",
    "print(\"Training data size:\", len(static_train_x))\n",
    "print(\"Test data size:\", len(static_test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2758688e4136ea84",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit model on single split\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m performance_measures \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_training_and_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_SVR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSVR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36mmodel_training_and_validation\u001b[0;34m(ml_model, name, splits, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m ml_model\u001b[38;5;241m.\u001b[39mfit(train_x, train_y)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate model performance results\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m accuracy, sens, spec, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, sens, spec, auc\n",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m, in \u001b[0;36mmodel_performance\u001b[0;34m(ml_model, test_x, test_y, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m ml_model\u001b[38;5;241m.\u001b[39mpredict(test_x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate the sensitivity\u001b[39;00m\n\u001b[1;32m     29\u001b[0m sens \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mrecall_score(test_y, pred_y)\n",
      "File \u001b[0;32m/lila/home/paynea/miniforge3/envs/harbor/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/lila/home/paynea/miniforge3/envs/harbor/lib/python3.10/site-packages/sklearn/metrics/_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/lila/home/paynea/miniforge3/envs/harbor/lib/python3.10/site-packages/sklearn/metrics/_classification.py:105\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    108\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "# Fit model on single split\n",
    "performance_measures = model_training_and_validation(model_SVR, \"SVR\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92730abaf7c150d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e8a1b6bda3e5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b17a1dd403270",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:harbor]",
   "language": "python",
   "name": "conda-env-harbor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
